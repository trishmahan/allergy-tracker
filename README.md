# allergy-tracker

I constantly struggle with allergy seasons, and wanted to create a daily pollen count alert system so that I’d know best when to take a daily vs. peak season allergy pill. While there are likely Alexa skills out there to set this up, I really wanted a timely, customized text message to be delivered, and never passing on an opportunity to use my own skills, I built my own.

First I tried using something simple, like [Zapier](http://zapier.com/) to implement this. Zapier has built in time/date based triggers, as well as easy to work with API Calls. However, when exploring my favorite allergy report website, they did not expose the pollen count through their open API. That data is still available, so I took a few simple steps to capture the data and turn it into a useful daily text message. 

* **Implementing a Web Scraper** - Python has a great library called [Beautiful Soup](https://pypi.org/project/beautifulsoup4/) that adds some default methods for scraping a webpage and parsing the response. I found a [great tutorial](https://realpython.com/python-web-scraping-practical-introduction/) on using Beautiful Soup, that helped me along the way to be able to pull the entire webpage I viewed everyday down to local code. 
* **Parsing the HTML Response** - Once I was able to retrieve the raw HTML locally, I needed to determine a way to get the variables stored outside of the HTML. This took a little work, and a lot of time reading through [parsing documentation](http://zetcode.com/python/beautifulsoup/) to determine how best to isolate the `<li>`s that contained the Tree, Grass, and Ragweed forecast. 
* **Sending an SMS Text Message** - I’d previously worked with Twilio, so it was my go to tool once I had data I wanted to send in a text message. Twilio also provides [great documentation](https://www.twilio.com/docs/sms/quickstart/python) in a variety of languages. 
* **Automating the Runtime** - Now that my code was sending me a text message everytime I ran it locally, I wanted to deploy it to a cloud server so that I could set up a Cron task that would run the code every day at 7am. I really like DigitalOcean’s flat cost structure, so I [set up my python environment](https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-programming-environment-on-an-ubuntu-18-04-server) on a droplet, and then [created the cron task](https://www.digitalocean.com/community/tutorials/how-to-use-cron-to-automate-tasks-ubuntu-1804) on that droplet. 

While this builds one simple tool for my very specific allergy use case, a lot of these capabilities can be used for a multitude of projects. This web scraper portion pulls allergy data right now, but could be modified by substituting a different URL to pull all sorts of data. The snippet function that sends an SMS could be used to send any message, and over time I actually had family members asking to be added to the text message distribution list to receive these updates. 

